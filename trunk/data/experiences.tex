% vim:tw=60

\section{Experiences}
\label{sec:exp}

\notes{any rational? What is the point? It is effective on
what? and why}

\subsection{Understanding runtime behaviour}
\notes{understanding cosmos client, en and csm, but the
dependency line may be missing. Because log doesn't contain
enough information to join tasks among threads.}

We try to use the mined data hierarchy from logs to
understand runtime behaviour of cosmos client and EN server.
Cosmos is a distributed storage system similiar to GFS. The
EN server is similiar to the role of chunkserver. In cosmos,
data is organized in streams and streams are comprised of
extents. Each stream and extent is identified by a GUID.

We choose (Session, opId) to analyze cosmos client runtime
and choose ExtentID to analyze EN server runtime.

The process of create a stream at client side is: (create a
stream), (append stream, append extent), (append extent),
(append extent and seal the last extent). Each parentheses
indicate a different opId, and all the 4 opId is in the same
session.

The process of create a stream at EN is: (create extent,
append extent x 3, seal extent). Using ExtentID, we can only
distinguish at extent operation level, no further precision.

lesson:
1) the choice of key is important. The result task model is
sensitive to the choice of key.

2) the task boundary has a phase-shift with real task
boundary. Because some preparation steps doesn't know the
key and value that early.

3) we can combine scalpel and data hierarchy together. Using
scalpel to automatically find dependency, and use data
hierarchy to find task boundary. The advantage is, data
hierarchy is predefined by system design and the result
should be more precise than bottom-up mining.

4) task model in this way is an overlay above low level
synchronization operations.

5) using ExtentID to analyze EN is too coarse.

\subsection{Guide on debugging}
\notes{
using the model to guide on debugging cosmos network
library.
}

Methodology: We have to identify the boundary of client
threads (who enqueue message) and worker threads (who really
send and receive message). So we send CsExtentInfo for a
random set of extents, so using ExtentID as key, we can
separate different tasks for both client threads and worker
threads.

Using our method we can only show that the worker threads
are working in a synchronized way. No overlay between tasks.
The root cause has to be invetigated manually.

We do it in three steps:

1. make sure client threads is faster than worker, so the
client threads are not bottleneck. We
maintain a flow control counter among client threads, and
see that it is alway near maximum value.

1.5. client threads enqueu the message and ignore it, means
we use asynchronous send setting.

2. worker's task segment is not interleaved, even for
different threads.

3. examine worker's processing model. We find that a
processor object is shared among different workers, so it is
a scarce resource. To send or receive a message, worker
threads must obtain the shared processor object to `run' the
message.

