% vim:textwidth=70:lines=42
\chapter{分布式管理系统的自管理方法研究}
\label{chap:selfman}

随着分布式系统规模越来越大，良好的管理它们也越来越困难。一个分布式管理
系统除了考虑设计合适的管理语意、支持复杂的管理功能，也需要考虑自身的自
管理、可扩展性、容错等问题。本文研究了分布式管理系统的自管理方法，提出
了基于epidemic算法和认证代理的解决方案。我们实现了一个可自管理的分布式
管理系统SMON，在Planet-Lab上的评测表明，SMON具有良好的性能与可扩展性。

%问题，相关工作，创新点，基本\textbf{方法}，实验

%问题提出：管理系统是一个分布式应用，同样需要管理
%
%目前的办法：集中部署启动管理系统到各个机器上，并集中式探测、维护（需要
%确认，或者说确定目前方法的缺陷）
%
%集中式的：plush，smartfrog，app-manager，globus，cfengine，还有
%
%plush, app-manager, smartfrog, cfengine, globus, amazon ec2
%
%==========
%为什么要管理管理系统：部署是必须的。更新是必须的。监测呢？可以用cron。
%
%ec2上可以用预装了plush的AMI
%
%攻击点：部署和更新也不经常有，监测死活可以用cron，管理节点死了几个也没事，
%早晚会发现，在那么大规模上，节点多几个少几个没太大问题。
%
%目前管理管理系统的办法有什么不足：
%
%一个可能得点：象self-host文章说的那样，说app希望可以改变运行的节点
%===========
%
%基本方法：节点相互探测对方状态，依据探测的结果，进行相应的管理步骤，实现
%
%系统的自管理。
%
%挑战：可扩展，鲁棒稳定，支持安全的自动认证，扩展管理能力(?)
%
%设计理念：简单、无状态(microreboot?)

\section{本章引言}

分布式系统~\cite{gfs, bigtable, dynamo}是当今Internet服务~
\cite{google, amazon}的关键组成部分。随着普通计算机硬件成本越来越低，
搭建一个规模成百上千的分布式计算平台是很普通的事情。一些常见的分布式计
算平台包括Planet-Lab~\cite{Bavier2004}, Amazon
EC2~\cite{Garfinkel2007} and Teragrid~\cite{Catlett2002}。这些计算平台
可以提供大容量的存贮能力和巨大的计算能力，设计良好的应用可以充
分利用这些资源来提升自己的性能。然而，有效的设计、部署和管理大规模分布
式应用以充分利用硬件资源仍然是一件困难的事情。

随着分布式应用的规模越来越大，有效的管理它们越来越有挑战性。分布式应用
管理系统能够帮助人们简化部署和维护分布式应用的管理工作。为了有效的完成
管理任务，管理系统也被设计为一个复杂的分布式系统。一个管理系统包括运行
在不同机器上的若干“节点\footnote{对于采用集中式算法的管理系统，节点就
是所谓的客户端}”。这些管理节点间形成一个覆盖网络，节点相互合作来部署
分布式应用。在大规模分布式系统上，故障或失败事件（failure）是经常发生，
包括节点故障、应用失败和网络故障。当分布式应用被部署并启动以后，管理系
统监测被管理应用的状态，并将应用从失败中恢复，恢复的方法依赖于特定应用
的语意。使用者也可以通过管理系统查询应用的状态，或者控制应用的运行。

我们注意到，由于管理系统实际上也是一个分布式系统，或者说分布式应用，这
引出了一个重要的问题：管理系统也需要被管理。使用者需要把管理节点部署到
一组机器上去，在管理系统被启动后，需要持续监测它的运行状态，并将它从失
败中恢复。

% 恢复管理系统的方法？

目前，人们使用集中式方式管理与维护需要使用的管理系统，然而集中式方式包
含很多缺点。首先，扩展性不好，效率低下。使用者从中心控制节点部署与维护
管理系统，对控制节点的网络资源消耗随系统规模增大而线性增长，在大规模情
况下，会对控制节点的网络资源带来很大负担。其次，集中式方式使用了星型的
静态拓扑结构，这种结构不能很好的应对网络环境的变化与失败事件，例如网络
分割（network partition）。

% 另外，不可能使用一个采用P2P方式的管理系统去管理其它管理系统。因为前
% 者同样需要被管理。

本文提出了可自管理的覆盖网络（SMON, Self-Managed Overlay
Network）。SMON是一个新颖的分布式应用管理系统（下文中简称管理系统，或
分布式管理系统），它具有内建自管理能力，包括自我部署、自我更新和自我恢
复能力。因而，它有效的解决了管理系统也需要被管理的问题。SMON包括分
布在一组机器上的管理节点，这些节点相互探测对方的状态，依据探测的结果，
自动执行自我管理与维护任务，包括安装新的SMON管理节点、更新SMON管理节点
至新的版本，或者恢复失败的SMON管理节点。这样，从整体来看，SMON具有了自
我管理的能力。同时，SMON提供了一组基本的管理语意。可以使用SMON部署与维
护其它分布式应用。

提出SMON的意义在于，如果我们把SMON与它管理的分布式应用看做一个整体，那
么这个新的分布式应用也具备了自管理的能力。特别的，如果被SMON管理的应用
是一个分布式应用管理系统，则那个管理系统也具备了自管理能力。也就是说，
SMON能够让任何分布式管理系统具备自管理能力。

设计SMON需要解决以下几个挑战性问题。

首先，SMON需要有良好的可扩展性。其次，为了让SMON节点能够自动在远程机器
上部署安装新的SMON节点，或者恢复运行失败的节点，需要有安全机制使SMON节点能够自
动与远程机器认证并登陆，同时保护认证信息（例如私钥）不会被泄露。最后，
SMON应该设计良好的应用管理语意。
% 最后，SMON的管理能力应该是能够被拓展的（extensible）。

SMON的设计分别处理了上面的问题，叙述如下。对第一个问题，SMON的节点构成
一个无结构的覆盖网络，节点间使用epidemic算法~\cite{Demers1987}相互探测，
并相互维护。epidemic算法保证了SMON系统具有良好的扩展性（$O(\log N)$），
同时epidemic算法也带来了额外的好处，首先它易于实现，其次，它使得SMON系
统很鲁棒，可以很好的应对网络分割（network partition）错误。对于第二个
问题，我们引入了认证代理机制来自动化SMON节点与远程机器认证。认证代理保
存着认证过程中需要的认证信息（例如私钥），它并不将认证信息泄露给任何
SMON节点，或其它外部对象。简单的说，SMON节点会将认证过程中收到的挑战密
文（challenge）转发给认证代理，并将认证代理回复的应答密文（response）
发送给远程机器，从而完成了自动认证的过程。对于第三个问题，SMON支持长期
运行的Internet服务管理语意，通过在SMON上部署新的管理系统，用户可以扩展
需要的其他管理语意。

% 对于第三个问题，可以使用SMON的自我更新功能，扩展它的管理能力。也可以
% 使用SMON部署其它管理系统。这样，我们既获得了需要的管理功能，也省去了
% 管理和维护的负担。

本章接来下的部分安排如下：\ref{sec:smon_design}节描述如何设计SMON系统，
包括如何实现自我管理，SMON的安全机制、邻居列表维护与应用管理语意的设计。
\ref{sec:smon_impl}节描述了如何在Planet-Lab上实现
SMON。\ref{sec:smon_app}节描述了SMON系统的一些应用方式。我们在理论上分
析SMON的性能（\ref{sec:smon_analysis}节），并在Planet-Lab上评测了SMON
系统的性能（\ref{sec:smon_eval}节），并将实测结果与理论分析结果进行了
对比。\ref{sec:smon_conclusion}节为本章小结。

\section{系统设计}
\label{sec:smon_design}

\begin{figure}[bthp]
  \centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{smon_arch}
    \caption[SMON系统结构]{SMON系统结构。SMON系统由三部分构成，
    SMON管理节点、认证代理和
    客户端工具。SMON节点构成SMON系统的主要部分，认证代理协助SMON节点与远程
    机器认证，使用者通过客户端工具与SMON通信}
    \label{fig:smon_arch}
  \end{minipage}
\end{figure}

SMON系统的体系结构如图~\ref{fig:smon_arch}所示，包含了三个部分：SMON节
点，认证代理和客户端工具。

SMON管理节点运行在分布式平台一组指定的机器上，每个节点都维护着完整的机
器列表，这个列表也称作是SMON节点的邻居列表。SMON节点使用epidemic算法相
互探测并相互维护。SMON节点定期从从邻居列表中随机选择一个节点，向它发
ping消息，如果超时没有收到回复的pong消息，那么这个SMON节点会试图向远程
机器安装新的SMON节点，如果远程机器已经安装有SMON节点，那么它将会试图恢
复失败的SMON节点。每个SMON节点都保存着自身的版本号，在用ping-pong消息
相互探测过程中，会顺便交换相互的版本号，低版本的SMON节点会将自身升级至
高版本。

认证代理持有与分布式平台上的机器认证所需的认证信息（例如私钥）。当
SMON节点在远程机器上部署新的SMON节点，或者恢复失败的SMON节点时，需要首
先与远程机器认证并登陆，这一过程需要认证代理的协助。

用户使用客户端工具来管理一组分布式应用，也可以通过客户端工具查询SMON与
应用的状态。

\subsection{自我管理设计}

\begin{figure}[bthp]
\centering
\begin{lstlisting}[language=Python,morekeywords={True,False},frame=tb,basicstyle=\small,numbers=right,numbersep=-5pt,numberstyle=\tiny]
while(1):
    if(livetag==False):
        continue

    p = random.choice(membership_list)
    try:
        remote_ver = ping(p, my_ver)
        if(remote_ver > my_ver):
            upgrade_myself()
    except Timeout:
        conn = Connect(p, auth_agent)
        if(conn.is_peer_installed()==True):
            conn.recover_peer()
        else:
            conn.deploy_peer()
        conn.close()

    sleep(WAIT_INTERVAL)
\end{lstlisting}
\caption{SMON节点运行伪代码}
\label{fig:peerflow}
\end{figure}

SMON节点的行为可以用图~\ref{fig:peerflow}中的伪代码描述。我们从第五行
开始解释，第2、3行的作用在后文中解释。SMON节点定期选择一个任意机器，并
向它发送ping消息（5-7行）。如果超时没有收到回应，它会尝试部署一个新的
节点或者恢复失败的节点（10-16行）。如果远程的SMON节点运行正常，则会返
回pong消息。ping/pong消息中包含两个通信的节点的版本号，它们会比较版本
的差异，并自动升级自己。

我们分别描述SMON自我管理的三个方面：自我部署，自我更新和自我恢复。

\subsubsection*{自我部署}

% 一些啰嗦的表达方式：
% SMON可以自动部署至一组节点，一组分布式节点
% 相互探测，相互维护
% ping -> liveness

SMON可以自动将自己部署至分布式计算平台的指定目标机器上。在最初状态时，
所有的目标机器上都没有被部署运行SMON节点。用户需要手动部署并启动一个
SMON节点。这个SMON节点会在邻居列表中随机选取一个机器，并在它上面部署第
二个SMON节点。新的SMON节点同样会在别的机器上部署SMON节点。当越来越多的
SMON节点被部署，自我部署过程会以幂指数速度增长。可以证明，以概率1，所
有可以连接的机器上都会被部署SMON节点~\cite{Eugster2004}。

详细来说，节点$P$会定期随机选择邻居列表里的一个机器$M$，并向它发送
SMON节点探测消息（ping）。如果超时没有收到回应的pong消息，$P$就会在$M$
上远程部署一份SMON节点。它首先在认证代理的协助下（在
\ref{subsec:security}节中描述）认证并登陆进$M$，向$M$远程拷贝一份SMON
的安装程序，并远程启动安装程序。启动的安装程序首先检查自己的完整性（使
用checksum校验和），以防传输过程中产生错误，然后它会安装并启动SMON节点。

在上述过程运行的任何时刻，都有可能因为外界因素而产生错误（例如连接中断，
机器崩溃等）。如果有错误发生，$P$会直接放弃这次远程部署的任务。
epidemic算法能够保证，在未来，$M$会被另外一个SMON节点$P'$选中并部署
SMON节点。

自我部署过程中存在着竞争情形（race condition）。因为采用了epidemic算法，
多个SMON节点可能同时试图向同一个机器$M$部署SMON节点。这个问题不需要
SMON节点间直接协调便可以解决。不同SMON节点会将安装程序远程复制到$M$上的
不同目录，相互不会覆盖。如果有多份安装程序被同时启动，它们会使用操作系
统提供的同步机制（例如lock file），保证只有一份安装程序能够运行。因此，
只有第一个运行的安装程序能够完成安装。在解决这个竞争情形的过程中，引入
了一定的额外开销，会有多份安装程序被拷贝至同一台机器。这会浪费一部分存
储资源和网络带宽。经过实验测试，可以看出额外开销的数量在多大数机器上是
很小的。同时考虑到安装程序很小，只有122KB，因此可以认为额外开销的影响
不大。

自我部署过程何时结束是难以严格确定的。理想情况是，当所有目标节点上都被
部署并运行着SMON节点时，自我部署过程就可以称为结束了。但是，在分布式环
境下，某些节点可能会发生错误，或者不可连接，所以，实际情况下自我部署过
程很难达到理想情况。即使一个机器已经被部署了SMON节点，也有可能因为机器
或者网络错误的原因，造成某些SMON节点失败或者不可连接。因此，为了应对
分布式环境频繁发生的异常错误，每个SMON节点需要不断的监测其它节点。自我
部署何时结束是一个需要用户定义的问题。用户可以查询有多少机器已经部署且
运行着SMON节点，根据需要决定自我部署是否结束。

\subsubsection*{自我更新}

SMON可以将自己自动更新至新的新的版本。这个更新过程是在线进行的，用户不
需要因为更新而停止整个SMON系统。

每个SMON节点都有版本号，版本号被持久保存在配置文件中。SMON节点会定期与
其它随机节点交换版本号。当发现相互的版本不同，版本低的SMON节点会自动从
高版本SMON节点那里获取新的安装程序，将自己更新至高版本。为了把整个SMON
系统更新至新版本，用户只需要更新一个SMON节点，整个SMON系统会逐渐收敛至
一个最终的版本。用户使用~\ref{subsec:client}节中描述的客户端工具更新任
何一个SMON节点。

存在着多个SMON节点向同一个版本SMON节点请求安装程序的可能，为了防止
瞬间拥挤（flash crowd）的冲击，节点会限制同时请求安装程序的数量。

\subsubsection*{自我恢复}

SMON系统可能遇到两种分布式环境中产生的错误：机器失败和网络分割。当一个
机器失败时，它上面运行的SMON节点也同时会被停止。SMON节点会尝试重新启动
失败的SMON节点，因此一旦失败的机器恢复，它上面的SMON节点会被重新启动。
同时启动多份SMON节点的实例是安全的，使用操作系统的同步机制，这等同于只
启动一个SMON实例。

网络分割也很容易应对。当网络分割发生时，SMON系统被分割为多个部分，每个
部分将会成为一个独立的小SMON系统，它们的状态，例如运行的SMON节点版本，
都将在各个部分内逐渐达成一致。当被分割的部分重新连接上时，不同部分的
节点会重新相互联系，从而整个系统的状态会再次收敛。

\subsubsection*{自我管理小结}

将上述三个部分合并起来，就构成了SMON自我管理的内容。SMON节点通过相互探
测并执行相应的管理维护任务来达到整个SMON系统自我管理的目标。有两点值得
说明。第一、作为一个优化，SMON间使用ping-pong消息探测时，会顺便交换
（piggyback）它们的版本号。第二、如果节点$A$认为节点$B$失败，可能有两
个原因，或者$B$没有运行，或者$B$所在机器没有部署SMON节点。$A$会首先判
断远程机器上是否部署了SMON，然后决定需要执行的动作。

\subsubsection*{禁止、启动自我管理功能}
\label{sec:disablesmon}

SMON的自我管理功能应该是能够被显式的禁止或者启动的。考虑如下场景：某个
研究人员使用SMON部署了一个分布式系统原型，在进行一些实验后，他决定停止
部署的分布式系统和SMON。如果SMON的自我管理功能无法被禁止，则整个SMON系
统不能够被停止。任何被用户停止的SMON节点，都很快会被其它SMON节点启动，
除非用户能够在一瞬间停止所有SMON节点，然而这在大规模分布式系统上是不可
能。

我们使用一个布尔变量\texttt{livetag}控制SMON节点的自我管理功能。如果它
的值是假，则节点停止定期探测其它节点的活动，因此整个系统停止了自我管理
的功能。这样，我们就能够逐一停止SMON节点，从而停止整个SMON系统了。

\texttt{livetag}变量有一个关联的版本号，每个SMON节点维护着变量的一个复
本。两个随机节点会定期交换\texttt{$<$livetag, version$>$}对，并且更新
\texttt{livetag}到最新的版本。需要注意到，节点交换\texttt{livetag}的行
为也是受到\texttt{livetag}值的控制的。当\texttt{livetag}值为假，节点不
会主动要求交换\texttt{$<$livetag, version$>$}对，但是仍然会回复来自其
它节点的消
息，这能够加快更新\texttt{livetag}的速度。

\subsection{安全机制}
\label{subsec:security}

SMON的安全机制需要保证如下两个目标：

\begin{itemize}

  \item 让节点能够自动与其它机器认证并登陆，从而节点能够自动安装新的
  SMON节点，或者远程恢复失败的SMON节点。

  \item 认证并加密SMON节点间的通信，不让系统被恶意利用，被用来部署
  恶意应用，例如botnet。

\end{itemize}

我们首先叙述如何在Planet-Lab上设计达到上述两个目标的安全机制，并解释如
何在其它分布式计算平台上应用这个机制。Planet-Lab使用公钥系统来认证对系
统平台的访问。每个用户使用ssh访问他的slice~\footnote{Planet-Lab分配给
用户的一组分布式资源，表现形式是一组分布式虚拟机。}。用户私钥保存在
自己的机器上，对应的公钥被分发到用户slice的所有虚拟机里。

\begin{figure}[bthp]
\centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{auth}
    \caption{SMON节点在认证代理协助下与远程机器认证并登陆的过程。图显示
    了这一过程中三者的交互过程}
    \label{fig:auth}
  \end{minipage}
\end{figure}

SMON引入了认证代理来协助节点自动与远程机器认证。认证代理持有用户私钥，
但私钥并不会被泄露。如图~\ref{fig:auth}所示，当SMON节点需要登陆到其它
机器上时，它首先连接远程机器的sshd服务器，并发送登陆请求。sshd服务器会
返回一个认证挑战密文，SMON节点没有用户私钥，无法求解对应的应答密文，它
会直接将挑战密文转发给认证代理，认证代理使用用户私钥求解得到应答密文，
并告诉SMON节点。SMON节点再将应答密文发送给sshd服务器。这样，SMON节点能
够登陆到用户slice内的任何一台机器上，并且用户私钥并未被泄露。

我们使用一个对称密钥$K_E$来认证和加密SMON节点之间、SMON节点和认证代理
之间的通信。$K_E$在SMON节点部署时一同被部署到各个机器上。由于部署的过
程中，通信是加密的，因此$K_E$不会被泄露。$K_E$也是可以安全保存在slice
内的机器上的，因为slice的虚拟机之间被很好的相互隔离。

用户应仔细评估并选择认证代理运行的机器，保证用户私钥的安全。认证代理不
必运行在用户slice内的某个机器上。如果SMON被部署的规模非常大，可以考虑
运行多份认证代理，提高认证的性能，同时有效的应对网络分割错误。SMON节点
可以在DNS服务器协助下~\cite{dns_akamai}，将选择离自己最近的认证代理。

上面叙述的安全机制可以推广应用到其它分布式平台上，如果分布式平台满足下
面的两个假设：

\begin{itemize}
  \item 使用挑战-回应类型的认证方式，例如公钥认证机制，或密码认证机制。
  
  \item $K_E$可以安全的与SMON节点一起保存。
\end{itemize}

这两个假设可以被很多分布式平台满足，因此使用认证代理的安全机制可以被广
泛应用，例如Amazon EC2平台。在EC2平台上，用户使用ssh和他的私钥来访问一
组分布式虚拟机，这与Planet-Lab平台的情形非常类似。另外，私有集群，例如
学校或者企业内部的集群也可以应用认证代理安全机制。在这些集群上，一般都
安装使用Linux系统，访问集群内机器的方式也是ssh加用户私钥。虽然集群不一
定使用虚拟机隔离用户的资源，但是系统的用户都是非恶意的，因此$K_E$也可
以安全的保存在各个机器的本地文件系统上。另外，可以设置文件权限等方法，
进一步保护$K_E$的安全。

\subsection{邻居列表维护}

SMON系统的邻居列表有其自身的特点。一般来说，在其它分布式系统中，邻居列
表维护着一组正在运行的节点，而在SMON中，邻居列表是一组应该运行着SMON节
点的机器。SMON节点失败并不会影响邻居列表的内容。这个列表由SMON的用户指
定，并且通常不会发生改变。另外SMON系统要求，即使只有一个SMON节点在运行，
它也应该能够知道所有其它的目标管理机器。这意味着，任何SMON节点，都应该
能够知道整个目标管理机器列表。

在目前的设计使用了一个简单而实用的方案：每个SMON节点都维护着整个目标机
器列表的一个复本，列表通过压缩的方式存储在各个机器上。邻居列表有关联的
版本号，SMON节点会用epidemic的方式随机交换邻居列表的版本号，并更新旧版
本的邻居列表。通过改变邻居列表，用户可以改变SMON运行在哪些机器上。

这个邻居列表维护方案足够应用在上万规模的系统上。由于列表中只需要包含一
组机器的IP地址或者主机名称，一个简单的计算表明，用zip格式存储15000+规
模的节点列表，只需要大约100K的存储空间，这是可以接受的。

当邻居列表改变时，新添加的机器上会被自动部署新的SMON节点。从邻居列表中
删除的机器上的SMON节点也会在与其它节点交换邻居列表版本时，更新自己的邻
居列表，从而得知自己从列表中被删除。但是，它仍旧会不断的向新邻居列表中
的节点发送ping消息。这样，就帮助加快了散播邻居列表改变的消息，同时帮助
在新添加的机器上部署新的SMON节点。当大部分SMON节点更新了它们的邻居列表
时，SMON系统已经迁移到了一组新的机器上，位于列表中被删除机器上的SMON节
点将会很少收到来自其它节点的消息，这些SMON节点会在长时间未收到消息的情
况下，自动停止自己的运行。

% 应用呢？

\subsection{应用管理}

SMON可以管理若干个分布式应用。管理语意可以从用下四个维度来定义，同时这
四个维度也是描述分布式应用管理需求的一般框架：

\begin{itemize}

\item 资源需求：不同的应用对计算资源有不同的要求，例如CPU负载，可用存
储与内存空间，网络带宽与延迟等等。除了描述资源当前使用情况的特征，用户
同时也对资源的可用性与稳定性有要求，这些特征可以通过资源历史数据推断得
出。管理系统应该可以发现符合应用管理规范的最佳资源。

\item 部署：管理系统根据用户的指令复制、解包并安装应用至获取的资源。管
理系统应当支持多种文件传输机制。

\item 同步：分布式应用可能分成多步完成，例如科学计算并行程序。管理系统
应当具有同步机制（barrier），从而支持将应用的工作流分步执行。

\item 维护与错误恢复：应用在执行时，资源使用情况可能发生变化，各种错误
也可能发生。管理系统应当动态监测资源状态，并将应用从错误中尽快恢复。

\end{itemize}

两类常见的分布式应用有长期运行的internet服务和grid方式的计算程序。它们
的管理需求小结在表~\ref{fig:app-man-req}

\begin{table*}[bthp]
\small
\centering
\caption{长期运行的internet服务和grid方式的计算程序的管理需求对比}
\label{fig:app-man-req}
\begin{tabular}{lp{5cm}p{5cm}}

\toprule[1.5pt]

 & 长期运行的internet服务 & grid方式的计算程序 \\

\midrule[1pt]

资源需求 & 

倾向于能够保证长期稳定的资源（例如几个月或者几年），从而提
供长期可持续的性能 

& 倾向于有强计算与存储能力的资源，稳定性要求是第二
位的，只要在计算时（通常几天）保证即可 \\

\midrule[1pt]

部署 & 平台依赖 & 平台依赖 \\

\midrule[1pt]

同步 & 应用节点可以相互独立启动，节点的启动顺序没有要求

& 并行程序需要几乎同时启动，同时计算可能分为若干步骤。\\
\midrule[1pt]

维护与错误恢复 

& 单个节点的错误可以被应用处理，节点应该被恢复以提供持
续的性能 

&
单个计算程序的错误可能导致整个计算失败 \\

\bottomrule[1.5pt]

\end{tabular}
\end{table*}

SMON的管理语意支持长期运行的internet服务。这类的服务包括P2P流媒体、
DHT和内容分发网络等。这些服务可能运行数月甚至数年，因而必须有很好的容
错性。在应用部署后，应用节点可以被相互独立的启动。节点相互组织成为一个
有结构或者随机结构的覆盖网络，并处理网络动态变化和错误。当节点失败时，
覆盖网络能够自动重新组织。失败的节点应当尽快被恢复，从而保证服务的可用
性和性能。

对于一个长期运行internet服务，SMON将应用部署至一组机器。机器的列表由用
户给出，用户可以通过资源发现服务寻找合适的机器列表。在每个机器安装应用
节点后立即启动应用，而不用等待其它其它应用节点的进度。每个SMON节点监测
并维护运行在同一个机器内的应用节点。在应用节点失败后，以重新启动的方式
恢复应用节点，而不用重新启动整个分布式应用。

由于分布式应用管理系统也属于长期运行的服务类别，我们可以通过在SMON上部
署新的管理系统过来扩充管理语意。这样，在管理功能被扩充的同时，将新的管
理系统和SMON合并来看，它们同时也保持了自管理的能力。我们在
~\ref{sec:smon_app}小节描述了扩充SMON的一些典型方式。

SMON使用epidemic算法部署应用。一个应用由独一无二的名字所标识。SMON节点
$A$定期与一个随机选择的SMON节点$B$交换它们管理的应用名称，如果发现一个
新的未部署的应用，$B$会从对方获取应用的安装程序，并在本机器部署安装新
的应用。

应用被部署并启动后，会被同一机器上运行的SMON节点所监测并维护。SMON从应
用管理规范中读取管理选项。用户可以给出应用节点的默认状态，并通过客户端
工具改变状态。应用可以有有三种指定状态：

\begin{itemize}
  \item 始终在线（online）：应用应始终在线，如果应用停止了，会被立即重
  新启动。

  \item 始终离线（offline）：应用应始终离线。运行的应用会被停止。

  \item 忽略（ignore）：仅在应用部署后启动应用，之后就不再监视应用的状
  态。
\end{itemize}

在应用管理规范中，用户应当制定三个脚本来启动、停止和重新启动应用。重启
脚本用来恢复失败的进程，在重新启动应用前可以执行一些清理步骤。SMON节点
会定期向应用指定的一个中心节点报告应用的状态。当应用的状态改变时，SMON
会立即向中心节点报告这个变化。

%

% 显然，SMON提供的应用监测与维护语意是很基本的。对于一些情况，这样的管
% 理语意是足够的，例如维护一个需要长期运行的分布式应用，应用的节点需要
% 尽可能处在运行状态。SMON提供的管理语意只考虑了应用是否运行，而没有进
% 一步监视应用是否运行在正确状态，包括应用的安全性与活性（safety and
% liveness）。为了扩展管理语意，增加新的管理功能，用户可以在SMON上部署
% 其它的分布式应用管理系统。这在~\ref{sec:smon_app}节有进一步论述。

\subsection{客户端工具}
\label{subsec:client}

用户可以用SMON的客户端工具来控制SMON。客户端工具和SMON节点之间的通信受
到对称密钥$K_E$认证和加密保护。为了部署一个新SMON系统，客户端工具会在
用户指定的一台机器上远程部署并启动一个SMON节点，之后，SMON会自动将自己
部署到用户指定的邻居列表内的机器上去。为了更新SMON系统，客户端工具将自
己模拟为一个SMON节点，并和任意一个SMON节点发送交换版本号消息，从而，那
个SMON节点会从客户端工具获取新的SMON安装程序，并更新自身，接下来，整个
SMON系统都会得到更新。类似的，用户可以使用客户端工具来部署新的分布式应
用程序，更新邻居列表，禁止与启动SMON的自我管理功能，等等。SMON系统的最
新版本号，以及邻居列表、\texttt{livetag}的最新版本号也由客户端工具维护，
集中保存在用户的机器上。最后，SMON节点实现了一组RPC，用户可以使用客户
端工具调用SMON的RPC接口，查询SMON节点的状态、应用的状态，以及管理分布
式应用，改变应用某个节点的维护选项。

\section{实现细节}
\label{sec:smon_impl}

我们针对Planet-Lab平台设计了SMON原型系统。Planet-Lab是一个全球范围的分
布式计算平台，它包含了超过400个站点，规模超过900个节点。使用
Planet-Lab的用户需要注册一个账号，并将账号与一个或多个slice相关联。
slice是Planet-Lab分配分布式资源的方式，一个slice对应分布在不同节点上的
一组虚拟机。用户可以使用ssh以公钥认证的方式登录他的slice里的机器。

我们使用Python语言实现SMON系统原型。最终的SMON节点安装程序大约122KB大
小，包括超过1000行代码。

我们使用单独的线程实现SMON节点运行时的epidemic活动。包括一个探测并维护
其它节点，实现自我管理功能的线程，一个更新邻居节点线程，一个更新
\texttt{livetag}的线程。对每一个应用，我们启动一个单独的管理与维护线程。

\begin{table*}
\small
\centering
\caption{SMON节点与认证代理支持的RPC调用语意}
\label{fig:smon_rpc}
%\begin{tabular}{|l|p{7cm}|}
\begin{tabular}{lp{7cm}}

\toprule[1.5pt]
\textbf{RPC} & \textbf{功能描述} \\

\midrule[1pt]
\texttt{ping(ver)} & ping其它SMON节点，并互换版本号\\

\texttt{retrieve\_peer(ver)} & 获取指定版本的SMON安装程序\\

\texttt{exchange\_livetag(tag, ver)} & 与其它SMON节点交换$<$livetag,
version$>$\\

\texttt{exchange\_member(ver)} & 与其它SMON节点交换邻居列表版本\\

\texttt{retreive\_member(ver)} & 获取指定版本的邻居列表\\

\texttt{exchange\_app(app\_name)} & 与其它SMON节点\\

\texttt{retrieve\_app(app\_name)} & 获取应用的安装程序\\

\texttt{set\_app\_status(app\_name, status)} & 设置应用状态(online,
offline, ignore)\\

\texttt{get\_app\_status()} & 查询应用状态\\

\texttt{resolve\_challenge(challenge)} & 求解挑战密文（认证代理实现）\\

\bottomrule[1.5pt]
\end{tabular}
\end{table*}

SMON节点间的通信使用RPC机制实现，总结在表~\ref{fig:smon_rpc}中。

SMON节点通过spawn一个ssh/scp进程来远程复制安装程序或者执行命令。我们使
用修改的\texttt{ssh-agent}将ssh认证时的挑战密文转发给认证代理，实现自
动认证的过程。认证代理实现了RPC接口\texttt{resolve\_challenge}，其输入
参数是认证的挑战密文，返回对应的应答密文。

SMON节点运行时的参数保存在一个配置文件中。包括连续epdemic活动之间的时
间间隔，SMON节点和邻居列表的版本号，$<$\texttt{livetag, version}$>$对，
以及认证代理的地址。每个应用也可以指定应用状态变化时目标报告节点的地址。
配置文件用SQLite数据库实现，保证数据不会在机器崩溃时丢失。配置文件由
SMON节点与应用的安装程序更新。邻居列表单独保存在一个压缩文件中。


\section{应用方式}
\label{sec:smon_app}

如图~\ref{fig:extend_smon}所示，我们叙述几种使用SMON的典型方式，同时也
是扩展SMON管理能力的典型方式：

\begin{figure}[htbp]
\centering
\begin{minipage}{0.8\linewidth}
\centering
\includegraphics[width=3in]{extend_smon}
\caption{通过在SMON上部署新的管理系统扩充管理语意}
\label{fig:extend_smon}
\end{minipage}
\end{figure}

首先，可以在SMON上部署其它分布式管理系统。SMON的管理语意针对长期运行的
Internet服务。这对目前常见的分布式系统都是适用的，然而并不适合Grid应用
管理。对这种情况，可以在SMON之上部署其它管理系统，以支持Grid上的任务流
语意。Plush就是一个可能的选择。Plush提供了控制分布式应用运行同步的语意，
可以控制应用在不同节点同时启动，并等待应用运行到相同的同步点
（barrier）后，再允许应用继续运行。这对于提供服务的分布式系统（例如CDN）
可能是不必要的，但是在Grid平台上，这能定义科学计算需要的工作流。

其次，可以扩展SMON的应用监测语意。SMON只监测了应用的外部状态：运行或停
止运行。为了监测系统内部的运行状态，可以部署D3S~\cite{d3s}。D3S使用插
装技术透明的获取系统内部状态，通过使用谓词，检查系统是否满足一定的
safety和liveness属性。由于谓词的计算考虑了分布在不同机器运行的进程，因
而D3S能够有效的发现系统是否满足或者违背系统设计预期的约束。一个典型的
例子是，对于一个分布式锁服务，D3S能够持续检查每个共享锁或者独享锁，是
否满足在任何情况下，都能够正确被共享或者独享的约束。

\section{性能分析}
\label{sec:smon_analysis}

在本节我们给出SMON系统的性能分析。SMON使用epidemic算法维护自身。节点定
期向随机节点发起通信并执行维护任务。我们接下来将会展示维护任务有良好的
性能可扩展性。在分析中，我们默认SMON以同步方式工作。也就是说，每个节点
以同步的方式在每一轮中完成维护任务，并且维护任务的执行时间非常小。虽然
实际的SMON系统工作在异步方式，但是这些分析仍然能够给出系统设计的性能特
征。同时，我们在实际的PlanetLab分布式平台上评测了SMON（
\ref{sec:smon_eval}节），验证了分析的正确性。

epidemic算法有三种变化，分别是push，pull和push-pull。不同的维护任务使
用不同的epidemic算法变种，如表~\ref{fig:smon_tasks}所示。虽然在细节上
有所不同，但是epidemic理论的基本结果显示，这些方法都会最终使所有节点“
被感染”。同时有，从一个节点开始，感染所有节点需要的时间的期望值是总人
数的对数（$\log(N)$）。

\begin{table}
\centering
\caption{SMON维护任务使用epidemic算法变种。}
\label{fig:smon_tasks}
\begin{tabular}{ll}

\toprule[1.5pt]
任务名称 & epidemic算法变种 \\
\midrule[1pt]
自我部署 & push \\

自我升级 & push-pull \\

自我恢复 & push \\

启动自管理 & push \\

停止自管理 & pull \\

应用部署 & push-pull \\

\bottomrule[1.5pt]
\end{tabular}
\end{table}


以自我部署过程为例。从一个SMON节点开始，所有其他目标机器都最终会被部署
SMON节点。依据已有理论~\cite{Bailey1975}，可以将这一过程建模为一个分支
过程，人口总数是$n$。如果每一轮每个节点都随机感染任意一个其他节点，也
就是部署一个SMON节点，在$r$轮之后，期望的被感染机器占总数的比例是
\begin{equation*}
Y_r \approx \frac{1}{1+ne^{-r}} 
\end{equation*}

$Y_r$和$r$的关系如图~\ref{fig:self-deploy_Yr}所示。

\begin{figure}
\centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{Y_r}
    \caption{$Y_r$（被感染人口比率）随$r$变化的曲线}
    \label{fig:self-deploy_Yr}
  \end{minipage}
\end{figure}

可以看出，已部署机器与未部署机器的比例随轮数$r$增加呈指数增长，平均来
说，每轮增长因数是$e$。这意味着自我部署过程具有良好的性能。

从另一方面看，部署固定比例的机器，期望的轮数$r$是
\begin{equation*}
r \approx \ln(n) - \ln(\frac{1}{Y_r} - 1)
\end{equation*}

$r$与总人数$n$呈对数关系，因而自我部署过程具有良好的可扩展性。

自我部署过程中的额外部署也是一个重要的性能参数。由于使用了epidemic算法，
在同一个机器上可能存在多个重复的SMON部署。虽然最终只会执行一个SMON节点
实例，但是重复的部署会在一定程度上浪费网络和存储资源。

接下来我们显示额外部署的数目是很小的，实际上是常数值。假设有$n$台机器
，其中$m, (m \le n)$台机器上已经部署了SMON节点。对于任意剩下的$n-m$个
机器来说，在下一轮可能会被$k, (k \le m)$个节点选中为部署目标。$k$的分
布满足参数是$(m, 1/n)$的二项分布
\begin{equation*}
P(X=k) = C_m^k (\frac{1}{n})^k (1 - \frac{1}{n})^{m - k}
\end{equation*}

定义同时发生的部署数目$k, (k > 0) $为额外部署数值，它的分布式是$k>0$时
的条件概率
\begin{equation*}
\begin{aligned}
P(X' = k) &= P(X=k | X > 0) \\
&= P(X = k) / (1 - P(X = 0)) \\ 
&= P(X = k) / (1 - (1 - \frac{1}{n})^{m}) \qquad (k > 0)
\end{aligned}
\end{equation*}

$k$的期望是
\begin{equation*}
\begin{aligned}
E(X') &= \sum_{k=1}^m k \cdot P(X=k|X>0)  \\
      &= \frac{1}{(1 - (1 - \frac{1}{n})^{m})} E(X)
\end{aligned}
\end{equation*}

进一步，我们注意到，当$n$很大，并且$m$接近$n$时，有
\begin{equation*}
\lim_{n \to \infty, m \to n} (1 - \frac{1}{n})^{m} = 1 / e
\end{equation*}

因而
\begin{equation*}
\begin{aligned}
E(X') &\approx 1.582 \cdot E(X) \\
      &= 1.582 \times \frac{m}{n}
\end{aligned}
\end{equation*}

最终有
\begin{equation*}
\lim_{n \to \infty, m \to n} E(X') = 1.582
\end{equation*}

这样显示了额外部署的数值在SMON大规模部署时趋近一个常数。

最后显示每个节点收到的ping消息也是常数。当SMON在$n$个机器上运行
时，任意一个SMON节点收到的ping消息数$p$满足参数是$(n, 1/n)$的二项分布
，因而
\begin{equation*}
E(p) = n \cdot 1/n = 1
\end{equation*}

\section{性能评测}
\label{sec:smon_eval}

我们在Planet-Lab平台上实际评测了SMON系统自我管理的性能，并将实验结果与
理论分析做对比。由于自我管理中的管理与维护任务都采用了epidemic算法，因
此选择其中典型操作进行测试：自我部署，禁止与启动自我管理。通过评
测能够看出，SMON系统的自我管理性能是优良的，并且有很好的可扩展性。
在自我部署过程中由于竞争情形引发的额外开销是足够小的，作为中心的认证代
理并不会成为性能瓶颈。

\subsection{性能}

首先评测SMON的性能。我们让SMON在117台Planet-Lab节点上完成自我部
署，并观测部署性能。在随机选取的一个Planet-Lab节点上，部署并启动了
一个SMON节点，自我部署过程开始。在目前设计中，SMON节点每隔5秒种，随机
探测其它1个节点。图~ \ref{fig:self-deploy}显示了部署过程的进度。我们可
以看出，90\%的节点，在95秒内被成功的部署了SMON节点。部署时间的中值是63
秒，最后一个节点被部署的时间是111秒。

\begin{figure}[htbp]
\centering
  \begin{minipage}{0.6\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{self-deploy117}
    \caption{SMON在117个Planet-Lab机器上自我部署的进度}
    \label{fig:self-deploy}
  \end{minipage}
\end{figure}

将理论分析的结果与实际结果做对比。$f=1$时，$Y_r$随$r$变化的关系如
图~\ref{fig:self-deploy_Yr}所示。可以看出，$Y_r$在小于大约50\%之前，增
长速度逐渐增快，在超过50\%之后，增长速度逐渐放慢。这与图~
\ref{fig:self-deploy}基本一致。直观的，这是因为在被“感染”人口小于
50\%时，随机选择一个人是未感染的概率大于50\%，因而病毒扩张的速度很快。
在很多人都感染后，随机选择一个人很有可能是已经感染病毒的，因而病毒扩张
的速度会放缓。

%与理论结果不同的是，自我部署的过程的“尾巴”要比图~\ref{fig:self-deploy_Yr}长。
%这是因为，一些机器由于负载太重，因此反应迟
%钝，另外连接至一些机器的网络速度太慢，因此这些机器上部署的过程比较慢。

进一步测试SMON的自我管理功能从禁止到启动的过程。让在117个
Planet-Lab节点上部署的SMON系统，从启动状态转换到禁止状态。图~
\ref{fig:state-transition}显示了转换的进度。对于90\%的节点来说，状态在
29秒后成功的转换，而状态转换的中值是20秒。可以看出，状态转换过程是非
常快的，与图~\ref{fig:self-deploy_Yr}的趋势也基本一致。

\begin{figure}[htbp]
\centering
  \begin{minipage}{0.6\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{state-trans117}
    \caption{SMON的自我管理功能从启动到禁止过程转换的进度}
    \label{fig:state-transition}
  \end{minipage}
\end{figure}

在自我部署过程中，存在着竞争情形，也就是多个SMON节点可能会在同一个机器
上部署新的SMON节点，这将引发额外开销。这个开销将会浪费网络和节点的存储
资源。计算了在每个机器上，发生的同时部署个数。结果显示在表~
\ref{fig:overhead}中。可以看出，有61.54\%的机器，只被部署了一次，对于
大部分机器，部署次数都在5以内，这样的机器覆盖了机器总数的98.3\%。平
均来说，每个机器被部署了1.88次，而部署次数在5以内的机器，它们的平均部
署次数是1.58。这样的结果表明，为解决竞争情形而引发的额外开销是可以接受
的。
\begin{table}[htbp]
\centering
\caption{额外负载分布}
\label{fig:overhead}
\begin{tabular}{lcc}
    \toprule[1.5pt]
额外负载 & 节点数 & \% \\
    \midrule[1pt]
1 & 72 & 61.54 \\
2 & 26 & 22.22 \\
3 & 12 & 10.26 \\
4 & 3 & 2.56 \\
5 & 2 & 1.71 \\
15 & 1 & 0.85 \\
23 & 1 & 0.85 \\
    \bottomrule[1.5pt]
\end{tabular}
\end{table}
需要注意到，使用分析得到的模型，每个节点被选中部署次数的平均值是1.582，
而实际是1.88。主要原因是，分析时实际上假设了部署新的SMON
节点可以瞬时完成，而在实际中需要一定的时间。因而，在一个机器被某个SMON
节点选中并部署时，仍然有别的SMON节点选中相同的机器，因此造成实际被选中
的次数大于理论分析的结果。

% scalability是符合log关系的。

\subsection{可扩展性}

其次评测自我部署过程的可扩展性，比较了在不同规模下部署SMON系统的性
能。在另外的24个节点上部署了新的SMON系统。表~\ref{fig:scalability}
总结并对比了在两个规模下的部署性能。从表中可以看出，两个系统规模的
比例是4.88（117/24），但是90\%进度的部署时间比例只有1.57（91/58），而
部署时间中值的比例是1.31（63/48）。这两个比值与两个系统规模对数的比值
近似相等（$\ln 117/\ln 24 = 1.49$）。可以看出，SMON系统具有良
好的扩展性。
\begin{table}[htbp]
\centering
  \begin{minipage}{0.8\linewidth}
    \centering
    \caption{在不同的规模SMON自我部署性能比较}
    \label{fig:scalability}
    \begin{tabular}{ccc}
    \toprule[1.5pt]
           & 24 nodes & 117 nodes\\
    \midrule[1pt]
    50\%完成 & 48 sec & 63 sec \\
    90\%完成 & 58 sec & 91 sec\\
    最终完成 & 61 sec & 111 sec\\
    \bottomrule[1.5pt]
    \end{tabular}
  \end{minipage}
\end{table}

\subsection{认证代理负载}

SMON使用认证代理帮助节点自动登陆到远程机器，以完成对其他节点的管理任务
。认证代理帮助求解挑战密文。求解的过程涉及了复杂的计算，因而需要确认认
证代理是否会成为性能瓶颈。

我们监测在SMON117个机器上自我部署时，认证代理上的CPU负载，并将结果小结
如图~\ref{fig:agentload}。认证代理运行在一台普通PC上，PC的配置是AMD双
核CPU 2.1GHz，2GB内存，运行Windows XP操作系统。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.6\linewidth]{cpuload}
\caption{认证代理上的CPU负载}
\label{fig:agentload}
\end{figure}

可以看出，CPU负载最高是13\%，通常低于10\%。认证代理上的负载是正比于认
证请求的频率的，而这一频率在一半机器被部署时达到最大，可以证明，这一最
大频率正比于系统的规模。因而可以得出，单个认证代理可以支撑将近千个
（$117 \times 100\%/13\%$）SMON节点完成自我部署。在SMON部署后，认证请
求的频率与节点失败的频率相关，通常情况下这个频率值较低。总的来说，认证
代理不会成为性能瓶颈。

\section{本章小结}
\label{sec:smon_conclusion}

本章研究了分布式应用管理系统自管理的问题。通过给管理系统增加内建的自管
理能力，从根本上解决了管理系统本身也需要被管理的问题。设计了一
个具备自管理能力的分布管理系统SMON（Self-Managed Overlay Network）。构
成这个系统的节点相互监测运行状态，并相互管理维护。SMON能自动将自己部署
到一组指定的节点上去，也能够自动恢复运行失败的节点，同时能够在线将整个
系统更新至新的版本。在Planet-Lab平台上对SMON进行了评测，结果显示，
SMON具有很好的扩展性和性能。
